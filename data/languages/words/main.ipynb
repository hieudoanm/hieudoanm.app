{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WordsAPI](https://www.wordsapi.com/) (325314)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import pandas\n",
    "import requests\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import urllib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(list, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(list), n):\n",
    "        yield list[i:i + n]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.wordsapi.com/mashape/words\"\n",
    "WHEN = \"2023-03-23T06:42:24.775Z\"\n",
    "ENCRYPTED = \"8cfdb189e722909be89207bfe958babbaeb2290937f892b8\"\n",
    "LIMIT = 1000000\n",
    "CHARACTERS = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b970ed3c16b54a3ca516324b94aa6cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for character in tqdm_notebook(CHARACTERS):\n",
    "    url = f\"{BASE_URL}?when={WHEN}&encrypted={ENCRYPTED}&limit={LIMIT}&letterPattern=^{character}\"\n",
    "    response = requests.request(\"GET\", url, timeout=10)\n",
    "    response_json = response.json()\n",
    "    data = response_json.get(\"results\", {}).get(\"data\", [])\n",
    "    words += data\n",
    "\n",
    "words.sort()\n",
    "\n",
    "with open('./words.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a34d45107084bd1bfffecc88f79b05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s 35970\n",
      "saw-leaved Error\n",
      "saw treesaw tooth Error\n",
      " Error\n",
      "saw wrack Error\n",
      "sawfly Error\n"
     ]
    }
   ],
   "source": [
    "words_file = open(\"./words.txt\", \"r\")\n",
    "words_string = words_file.read()\n",
    "words = words_string.split(\"\\n\")\n",
    "\n",
    "def get_word(word : str):\n",
    "    try:\n",
    "        encoded_word = urllib.parse.quote(word, safe='')\n",
    "        url = f\"{BASE_URL}/{encoded_word}?when={WHEN}&encrypted={ENCRYPTED}\"\n",
    "        response = requests.request(\"GET\", url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            response_text = response.text\n",
    "            return response_text\n",
    "        else:\n",
    "            print(word, \"Error\")\n",
    "            return \"\"\n",
    "    except:\n",
    "        print(word, \"Error\")\n",
    "        return \"\"\n",
    "\n",
    "characters_by_count = []\n",
    "\n",
    "total = 0\n",
    " \n",
    "for character in tqdm_notebook(CHARACTERS):\n",
    "    words_with_x = list(filter(lambda word: word[0] == character, words))\n",
    "    count = len(words_with_x)\n",
    "    total += count\n",
    "    characters_by_count.append({\n",
    "        \"character\": character,\n",
    "        \"count\": count,\n",
    "    })\n",
    "\n",
    "characters_by_count.append({\n",
    "    \"character\": \"total\",\n",
    "    \"count\": total,\n",
    "})\n",
    "\n",
    "sorted_characters_by_count = sorted(characters_by_count, key=lambda d: d['count'])\n",
    "\n",
    "characters_by_count_data_frame = pandas.DataFrame.from_dict(sorted_characters_by_count)\n",
    "characters_by_count_data_frame.to_csv('./characters.csv', index = False, header = True)\n",
    "\n",
    "def get_words_with_x(words, character : str):\n",
    "    return list(filter(lambda word: word[0] == character, words))\n",
    "\n",
    "for item in [{ \"character\": \"s\", \"count\": 35970 }]:\n",
    "    character = item.get(\"character\", \"\")\n",
    "    words_with_x = get_words_with_x(words, character)\n",
    "    count = item.get(\"count\", 0)\n",
    "    words_with_x_with_details = []\n",
    "    chunks_words_with_x = chunks(words_with_x, 1000)\n",
    "    print(character, count)\n",
    "    for chunk_words_with_x in chunks_words_with_x:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for word in chunk_words_with_x:\n",
    "                futures.append(executor.submit(get_word, word=word))\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                word_details = future.result()\n",
    "                if word_details != \"\":\n",
    "                    words_with_x_with_details.append(word_details)\n",
    "        words_with_x_with_details = list(set(words_with_x_with_details))\n",
    "        words_with_x_with_details.sort()\n",
    "        with open(f'./jsonl/{character}.jsonl', 'w') as file_open:\n",
    "    \t    file_open.write('\\n'.join(words_with_x_with_details))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bc85b664864989b2cdfaf8ba17bef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "collected_characters_by_count = []\n",
    "\n",
    "collected_total = 0\n",
    "\n",
    "words_with_results = []\n",
    "\n",
    "for character in tqdm_notebook(CHARACTERS):\n",
    "    file_open = open(f\"./jsonl/{character}.jsonl\", \"r\")\n",
    "    content : str = file_open.read()\n",
    "    json_lines : list[str] = list(filter(lambda line: line != \"\", content.split(\"\\n\")))\n",
    "    count = len(json_lines)\n",
    "    collected_total += count\n",
    "    collected_characters_by_count.append({\n",
    "        \"character\": character,\n",
    "        \"collected\": count\n",
    "    })\n",
    "    for json_line in json_lines:\n",
    "        json_object = json.loads(json_line)\n",
    "        word = json_object.get(\"word\", \"\")\n",
    "        results = json_object.get(\"results\", [])\n",
    "        if len(results) > 0:\n",
    "            words_with_results.append(word)\n",
    "\n",
    "collected_characters_by_count.append({\n",
    "    \"character\": \"total\",\n",
    "    \"collected\": collected_total,\n",
    "})\n",
    "\n",
    "sorted_characters_by_count = sorted(collected_characters_by_count, key=lambda d: d['character'])\n",
    "\n",
    "collected_characters_by_count_data_frame = pandas.DataFrame.from_dict(sorted_characters_by_count)\n",
    "merged_data_frame = characters_by_count_data_frame.merge(collected_characters_by_count_data_frame, how=\"inner\", on=\"character\")\n",
    "merged_data_frame.to_csv('./characters.csv', index = False, header = True)\n",
    "\n",
    "with open('./words_with_results.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(words_with_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
